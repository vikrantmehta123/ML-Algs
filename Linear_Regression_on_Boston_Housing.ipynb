{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vikrantmehta123/ML-Algs/blob/main/Linear_Regression_on_Boston_Housing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Linear Regression on Boston Housing Dataset"
      ],
      "metadata": {
        "id": "eZh2A58VWrrB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Getting the Dataset:\n",
        "The below cell gets the dataset from keras library and makes the split into train and test data.\n",
        "* `Training_data` = Training data matrix of shape $(n, d)$\n",
        "* `labels` = label vector corresponding to the training data\n",
        "* `test_data` = Test data matrix of shape $(n_1, d)$ where $n_1$ is the number of examples in test dataset.\n",
        "* `test_labels` = label vector corresponding to the test data"
      ],
      "metadata": {
        "id": "WT2nf7VJXHsH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0kgvCN0n3_1p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc607812-faf0-4c45-8308-232806736cf7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/boston_housing.npz\n",
            "57026/57026 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from keras.datasets import boston_housing\n",
        "\n",
        "Train, test = boston_housing.load_data(seed= 111)\n",
        "Training_data, labels = Train[0], Train[1]\n",
        "Test_data, test_labels = test[0], test[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Add Dummy Feature to the Dataset\n",
        "\n",
        "Run the cell below to add a dummy feature in the feature matrix `Training_data` and test data matrix `test_data`."
      ],
      "metadata": {
        "id": "kN5fsfqfmX_w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dummy_feature_train, dummy_feature_test = np.ones(Training_data.shape[0]), np.ones(test_data.shape[0])\n",
        "X, X_test = np.column_stack((dummy_feature_train, Training_data)), np.column_stack((dummy_feature_test, test_data))\n",
        "X = X.T\n",
        "X.shape"
      ],
      "metadata": {
        "id": "wic4nhW47fOv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 1: Normal Equation"
      ],
      "metadata": {
        "id": "RIcn6ecgav6y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Weight Vector $\\vec{w}$\n",
        "\n",
        "Using the normal equation for linear regression, compute the weight vector as $$ \\vec{w} = (X \\cdot X^T)^{-1} \\cdot (X \\cdot y) $$\n",
        "\n"
      ],
      "metadata": {
        "id": "sK4oWgqCnzgc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w = np.linalg.pinv(X @ X.T) @ X @ labels"
      ],
      "metadata": {
        "id": "JORYNRkdOo55"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prediction\n",
        "\n",
        "Using $X^T \\cdot \\vec{w} $, predict on the training data.\n"
      ],
      "metadata": {
        "id": "uBUor5KWp3_Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = X.T @ w"
      ],
      "metadata": {
        "id": "aXdIjOFHl78K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Loss\n",
        "\n",
        "Using the predictions computed in the above cell, compute the loss on the training data. Consider the loss to be defined as:\n",
        "\n",
        "$$ \\sqrt{\\dfrac{1}{n}\\sum\\limits_{i=1}^{n} (y_i- \\hat{y}_i)^2}\n",
        "$$\n",
        "\n",
        "Where $\\hat{y}_i$ is the prediction for $i^{th}$ data point. \n",
        "\n"
      ],
      "metadata": {
        "id": "FSDbBz7ucm_a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss = np.sqrt(np.mean((labels - predictions) ** 2))\n",
        "loss"
      ],
      "metadata": {
        "id": "F4jRui2VSeDt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test Loss\n",
        "\n",
        "Predict on the test dataset and compute the test loss. Consider the loss to be defined as\n",
        "\n",
        "$$ \\sqrt{\\dfrac{1}{n}\\sum\\limits_{i=1}^{n} (y_i- \\hat{y}_i)^2}\n",
        "$$\n",
        "\n",
        "Where $\\hat{y}_i$ is the prediction for $i^{th}$ data point. \n",
        "\n"
      ],
      "metadata": {
        "id": "5eh8cI4PeVEb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_predictions = X_test @ w\n",
        "test_loss = np.sqrt(np.mean((test_labels - test_predictions) ** 2))\n",
        "test_loss"
      ],
      "metadata": {
        "id": "vBMCgEIBU6v5",
        "outputId": "d54a7468-ebf7-4e5b-8d85-f91722bbf7a2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5.327662216177319"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 2: Gradient Descent\n",
        "\n",
        "Find the weight vector using the gradient descent. Here a constant learning rate of $\\eta = 10^{-10}$ is used, and the number of iterations is taken as 100.\n",
        "\n"
      ],
      "metadata": {
        "id": "NkeClcplfJLK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent(X, y, iters, w, lr):\n",
        "    for i in range(iters):\n",
        "        w = w - lr * (2 * (X @ X.T) @ w  -  2 * X @ y)\n",
        "    return w"
      ],
      "metadata": {
        "id": "QbpyGnfgWEqj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w = np.zeros(X.shape[0])\n",
        "w = gradient_descent(X, labels, 100, w, 1e-10)\n",
        "w"
      ],
      "metadata": {
        "id": "rI-nOZZXqDQ0",
        "outputId": "922ef177-6f84-4f03-b755-246df325b34a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.058959061195902614"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Loss\n",
        "\n",
        "Run the cell below to find the loss for the training data points for the model generated by gradient descent. The loss to be defined as\n",
        "\n",
        "$$ \\sqrt{\\dfrac{1}{n}\\sum\\limits_{i=1}^{n} (y_i- \\hat{y}_i)^2}\n",
        "$$\n",
        "\n",
        "Where $\\hat{y}_i$ is the prediction for $i^{th}$ data point. \n",
        "\n"
      ],
      "metadata": {
        "id": "78ApH0oAg96X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = X.T @ w\n",
        "np.sqrt(np.mean((predictions - labels)**2))"
      ],
      "metadata": {
        "id": "oI1yIf9N8la7",
        "outputId": "7fb3d052-8517-4dd6-f389-37021acdf697",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11.13727323702196"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test Loss\n",
        "\n",
        "Run the cell below to find the loss for the test data points for the model learnt using the gradient descent. The loss to be defined as\n",
        "\n",
        "$$ \\sqrt{\\dfrac{1}{n}\\sum\\limits_{i=1}^{n} (y_i- \\hat{y}_i)^2}\n",
        "$$\n",
        "\n",
        "Where $\\hat{y}_i$ is the prediction for $i^{th}$ data point. \n",
        "\n"
      ],
      "metadata": {
        "id": "FA7UKT1Y3PXa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_predictions = X_test @ w\n",
        "np.sqrt(np.mean((test_labels - test_predictions)**2))"
      ],
      "metadata": {
        "id": "tevVzIIj3SZj",
        "outputId": "47e731db-04df-4335-b44e-2284975fc6fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10.964491250062146"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 3: Stochastic Gradient Descent\n",
        "\n",
        "Run the cells below to find the weight vector using stochastic gradient descent. A constant learning rate of $\\eta = 10^{-10}$ is being used and the number of iterations is taken as 1000, whereas the batch size is taken as $⌈\\text{number of samples}/5⌉ $. For sampling the batch examples in $ith$ iteration, seed is set at $i$. The final weight vector is the last updated weight vector.\n",
        "\n"
      ],
      "metadata": {
        "id": "-AoLsBKc31Ul"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def stochastic_gradient_descent(X, y, w, batch_size,iters, lr):\n",
        "    for i in range(iters):\n",
        "        rng = np.random.default_rng(seed=i)\n",
        "        indices = rng.integers(0, X.shape[1], size=batch_size)\n",
        "        samples = X[: , indices]\n",
        "        sample_labels = y[indices]\n",
        "        w = w - lr * (2 * (samples @ samples.T) @ w  -  2 *(samples @ sample_labels))\n",
        "    return w\n"
      ],
      "metadata": {
        "id": "R4odop9yF9VY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialise\n",
        "batch_size = (X.shape[1])//5 \n",
        "w = np.zeros(X.shape[0])\n",
        "\n",
        "w = stochastic_gradient_descent(X, labels, w, batch_size, 1000, 1e-10)\n",
        "w"
      ],
      "metadata": {
        "id": "rfBgH2njuIaQ",
        "outputId": "daa378b3-dec1-4993-a19a-3130d55bbfb8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.10124206187312643"
            ]
          },
          "metadata": {},
          "execution_count": 157
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Loss:\n",
        "\n",
        "Run the cell to find the loss for the training data points if the model is learnt using the stochastic gradient descent. Consider the loss to be defined as\n",
        "\n",
        "$$ \\sqrt{\\dfrac{1}{n}\\sum\\limits_{i=1}^{n} (y_i- \\hat{y}_i)^2}\n",
        "$$\n",
        "\n",
        "Where $\\hat{y}_i$ is the prediction for $i^{th}$ data point. \n",
        "\n"
      ],
      "metadata": {
        "id": "yPzJLciH4NrV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_predictions = X.T @ w\n",
        "np.sqrt(np.mean((labels - train_predictions) ** 2))"
      ],
      "metadata": {
        "id": "w9usLAPeLNkt",
        "outputId": "9a941a6f-59b7-4224-9c15-8eef14cd75df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8.6246359251934"
            ]
          },
          "metadata": {},
          "execution_count": 160
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test Loss\n",
        "\n",
        "Run the cell below to find the loss for the test data points if the model is learnt using the stochastic gradient descent. Consider the loss to be defined as\n",
        "\n",
        "$$ \\sqrt{\\dfrac{1}{n}\\sum\\limits_{i=1}^{n} (y_i- \\hat{y}_i)^2}\n",
        "$$\n",
        "\n",
        "Where $\\hat{y}_i$ is the prediction for $i^{th}$ data point. \n"
      ],
      "metadata": {
        "id": "rfeamQM94x_N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = X_test @ w\n",
        "np.sqrt(np.mean((test_labels - predictions) ** 2))"
      ],
      "metadata": {
        "id": "oF1xpNH845iH",
        "outputId": "8686316b-3091-47a4-e8cd-9ee5f8040eaf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8.352305624451189"
            ]
          },
          "metadata": {},
          "execution_count": 162
        }
      ]
    }
  ]
}